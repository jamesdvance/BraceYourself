{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "u8Ndq0CG_qBV"
      },
      "source": [
        "<td>\n",
        "   <a target=\"_blank\" href=\"https://labelbox.com\" ><img src=\"https://labelbox.com/blog/content/images/2021/02/logo-v4.svg\" width=256/></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "maxyrS0i_qBW"
      },
      "source": [
        "<td>\n",
        "<a href=\"https://colab.research.google.com/github/Labelbox/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<a href=\"https://github.com/Labelbox/labelbox-python/blob/master/examples/integrations/sam/meta_sam_labelbox.ipynb\" target=\"_blank\"><img\n",
        "src=\"https://img.shields.io/badge/GitHub-100000?logo=github&logoColor=white\" alt=\"GitHub\"></a>\n",
        "</td>"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "q0j6Umjw_qBX"
      },
      "source": [
        "# Predicting bounding boxes around common objects using YOLOv8\n",
        "\n",
        "First, we start with loading the YOLOv8 model, getting a sample image, and running the model on it to generate bounding boxes around some common objects."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "tSWlotKS_qBX",
        "outputId": "f783a4e0-8d3e-4a11-88a7-e3eeb08cfc15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 24 14:05:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    23W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "VMlvhm1B_qBY",
        "outputId": "6dff33a4-64d8-4492-fc13-84231c89aabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "metadata": {
        "id": "9101Nw___qBY",
        "outputId": "d778f4b4-15ee-4daf-f310-f4a1da79afd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Pip install method (recommended)\n",
        "!pip install ultralytics==8.0.20\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n",
            "Setup complete âœ… (8 CPUs, 51.0 GB RAM, 27.1/166.8 GB disk)\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "2vL0AXcn_qBY"
      },
      "source": [
        "Here we run inference on the image using the YOLOv8 model."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "k7PpNugc_qBY",
        "outputId": "ba6e6aa5-d123-4ed4-a164-405487aa54fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# You can also use the Labelbox Client API to get specific images or an entire\n",
        "# dataset from your Catalog. Refer to these docs:\n",
        "# https://labelbox-python.readthedocs.io/en/latest/#labelbox.client.Client.get_data_row\n",
        "\n",
        "IMAGE_PATH = \"https://storage.googleapis.com/labelbox-datasets/image_sample_data/chairs.jpeg\"\n",
        "\n",
        "!wget -v {IMAGE_PATH}"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-24 14:05:40--  https://storage.googleapis.com/labelbox-datasets/image_sample_data/chairs.jpeg\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.207, 108.177.126.207, 108.177.127.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503155 (491K) [image/jpeg]\n",
            "Saving to: â€˜chairs.jpegâ€™\n",
            "\n",
            "chairs.jpeg         100%[===================>] 491.36K  1.13MB/s    in 0.4s    \n",
            "\n",
            "2023-10-24 14:05:41 (1.13 MB/s) - â€˜chairs.jpegâ€™ saved [503155/503155]\n",
            "\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "k5JEkYPG_qBY",
        "outputId": "e86a10d7-2e06-4b9f-91ea-90d67639eae4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = YOLO(f'{HOME}/yolov8n.pt')\n",
        "results = model.predict(source=\"{}/{}\".format(HOME, os.path.basename(IMAGE_PATH)), conf=0.25)\n",
        "\n",
        "# print(results[0].boxes.xyxy) # print bounding box coordinates\n",
        "\n",
        "# print(results[0].boxes.conf) # print confidence scores\n",
        "\n",
        "#for c in results[0].boxes.cls:\n",
        "# print(model.names[int(c)]) # print predicted classes"
      ],
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to /content/yolov8n.pt...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 167MB/s]\n",
            "\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla V100-SXM2-16GB, 16151MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "IwJfWuD7_qBY"
      },
      "source": [
        "Here we visualize the bounding boxes on the image."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "b5NsVD68_qBY"
      },
      "source": [
        "image_bgr = cv2.imread(\"{}/{}\".format(HOME, os.path.basename(IMAGE_PATH)), cv2.IMREAD_COLOR)\n",
        "\n",
        "for box in results[0].boxes.xyxy:\n",
        "  cv2.rectangle(image_bgr, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
        "\n",
        "cv2_imshow(image_bgr)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "udjwnIuP_qBZ"
      },
      "source": [
        "# Predicting segmentation masks using Meta's Segment Anything model\n",
        "\n",
        "Now we load Meta's Segment Anything model and feed the bounding boxes to it, so it can generate segmentation masks within them."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "4-_r6TwS_qBZ"
      },
      "source": [
        "%cd {HOME}\n",
        "\n",
        "# Download SAM model SDK\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "kSc-FomZ_qBZ"
      },
      "source": [
        "# Download SAM model weights\n",
        "\n",
        "%cd {HOME}\n",
        "!mkdir {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "vgTLRpog_qBZ"
      },
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-qZyJnk1_qBZ"
      },
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_h\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "53_f1Ige_qBZ"
      },
      "source": [
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "mask_predictor = SamPredictor(sam)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "p4nUnqv9_qBZ"
      },
      "source": [
        "image_bgr = cv2.imread(\"{}/{}\".format(HOME, os.path.basename(IMAGE_PATH)), cv2.IMREAD_COLOR)\n",
        "\n",
        "transformed_boxes = mask_predictor.transform.apply_boxes_torch(results[0].boxes.xyxy, image_bgr.shape[:2])\n",
        "\n",
        "mask_predictor.set_image(image_bgr)\n",
        "\n",
        "masks, scores, logits = mask_predictor.predict_torch(\n",
        "    boxes = transformed_boxes,\n",
        "    multimask_output=False,\n",
        "    point_coords=None,\n",
        "    point_labels=None\n",
        ")\n",
        "masks = np.array(masks.cpu())\n",
        "\n",
        "# print(masks)\n",
        "# print(scores)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YHBHnQgp_qBZ"
      },
      "source": [
        "Here we visualize the segmentation masks drawn on the image."
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "3qBSPe9h_qBZ"
      },
      "source": [
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "final_mask = None\n",
        "for i in range(len(masks) - 1):\n",
        "  if final_mask is None:\n",
        "    final_mask = np.bitwise_or(masks[i][0], masks[i+1][0])\n",
        "  else:\n",
        "    final_mask = np.bitwise_or(final_mask, masks[i+1][0])\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis('off')\n",
        "plt.imshow(final_mask, cmap='gray', alpha=0.7)\n",
        "\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "TvB5pbxi_qBa"
      },
      "source": [
        "# Uploading predicted segmentation masks with class names to Labelbox using Python SDK"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "LeX6o8LI_qBa"
      },
      "source": [
        "# Install labelbox package\n",
        "\n",
        "!pip install -q \"labelbox[data]\""
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "_rKdC_le_qBa"
      },
      "source": [
        "import uuid\n",
        "import numpy as np\n",
        "import labelbox as lb\n",
        "import labelbox.types as lb_types"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GgNv8vaY_qBa"
      },
      "source": [
        "# Create a Labelbox API key for your account by following the instructions here:\n",
        "# https://docs.labelbox.com/reference/create-api-key\n",
        "# Then, fill it in here\n",
        "\n",
        "API_KEY = \"\"\n",
        "client = lb.Client(API_KEY)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "21G0dMh6_qBa"
      },
      "source": [
        "# Create a Labelbox ObjectAnnotation of type mask for each predicted mask\n",
        "\n",
        "# Identifying what values in the numpy array correspond to the mask annotation\n",
        "color = (1, 1, 1)\n",
        "\n",
        "class_names = []\n",
        "for c in results[0].boxes.cls:\n",
        "  class_names.append(model.names[int(c)])\n",
        "\n",
        "annotations = []\n",
        "for idx, mask in enumerate(masks):\n",
        "  mask_data = lb_types.MaskData.from_2D_arr(np.asarray(mask[0], dtype=\"uint8\"))\n",
        "  mask_annotation = lb_types.ObjectAnnotation(\n",
        "    name = class_names[idx], # this is the class predicted in Step 1 (object detector)\n",
        "    value=lb_types.Mask(mask=mask_data, color=color),\n",
        "  )\n",
        "  annotations.append(mask_annotation)\n",
        "  print(mask_annotation)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "JIfkk9Td_qBa"
      },
      "source": [
        "# Create a new dataset\n",
        "\n",
        "# read more here: https://docs.labelbox.com/reference/data-row-global-keys\n",
        "global_key = \"my_unique_global_key\"\n",
        "\n",
        "test_img_url = {\n",
        "    \"row_data\": IMAGE_PATH,\n",
        "    \"global_key\": global_key\n",
        "}\n",
        "\n",
        "dataset = client.create_dataset(name=\"auto-mask-classification-dataset\")\n",
        "task = dataset.create_data_rows([test_img_url])\n",
        "task.wait_till_done()\n",
        "\n",
        "print(f\"Errors: {task.errors}\")\n",
        "print(f\"Failed data rows: {task.failed_data_rows}\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "N12lwGlN_qBa"
      },
      "source": [
        "# Create a new ontology if you don't have one\n",
        "\n",
        "# Add all unique classes detected in Step 1\n",
        "tools = []\n",
        "for name in set(class_names):\n",
        "  tools.append(lb.Tool(tool=lb.Tool.Type.RASTER_SEGMENTATION, name=name))\n",
        "\n",
        "ontology_builder = lb.OntologyBuilder(\n",
        "    classifications=[],\n",
        "    tools=tools\n",
        "  )\n",
        "\n",
        "ontology = client.create_ontology(\"auto-mask-classification-ontology\",\n",
        "                                  ontology_builder.asdict(),\n",
        "                                  media_type=lb.MediaType.Image\n",
        "                                  )\n",
        "\n",
        "# Or get an existing ontology by name or ID (uncomment one of the below)\n",
        "\n",
        "# ontology = client.get_ontologies(\"Demo Chair\").get_one()\n",
        "\n",
        "# ontology = client.get_ontology(\"clhee8kzt049v094h7stq7v25\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "0uX_woBO_qBa"
      },
      "source": [
        "# Create a new project if you don't have one\n",
        "\n",
        "# Project defaults to batch mode with benchmark quality settings if this argument is not provided\n",
        "# Queue mode will be deprecated once dataset mode is deprecated\n",
        "project = client.create_project(name=\"auto-mask-classification-project\",\n",
        "                                media_type=lb.MediaType.Image\n",
        "                                )\n",
        "\n",
        "# Or get an existing project by ID (uncomment the below)\n",
        "\n",
        "# project = get_project(\"fill_in_project_id\")\n",
        "\n",
        "# If the project already has an ontology set up, comment out this line\n",
        "project.setup_editor(ontology)\n",
        "\n",
        "print(project.uid)"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e-xjQBoc_qBa"
      },
      "source": [
        "# Create a new batch of data for the project you specified above\n",
        "\n",
        "data_row_ids = client.get_data_row_ids_for_global_keys([global_key])['results']\n",
        "\n",
        "batch = project.create_batch(\n",
        "    \"auto-mask-classification-batch\",  # each batch in a project must have a unique name\n",
        "    data_rows=data_row_ids,\n",
        "\n",
        "    # you can also specify global_keys instead of data_rows\n",
        "    #global_keys=[global_key],  # paginated collection of data row objects, list of data row ids or global keys\n",
        "\n",
        "    priority=1  # priority between 1(highest) - 5(lowest)\n",
        ")\n",
        "\n",
        "print(f\"Batch: {batch}\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "lFHCYc2d_qBa"
      },
      "source": [
        "labels = []\n",
        "labels.append(\n",
        "    lb_types.Label(data=lb_types.ImageData(global_key=global_key),\n",
        "                   annotations=annotations))"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Xm6uIbIJ_qBa"
      },
      "source": [
        "# Upload the predictions to your specified project and data rows as pre-labels\n",
        "\n",
        "upload_job = lb.MALPredictionImport.create_from_objects(\n",
        "    client=client,\n",
        "    project_id=project.uid,\n",
        "    name=\"mal_job\" + str(uuid.uuid4()),\n",
        "    predictions=labels\n",
        ")\n",
        "upload_job.wait_until_done()\n",
        "\n",
        "print(f\"Errors: {upload_job.errors}\", )\n",
        "print(f\"Status of uploads: {upload_job.statuses}\")"
      ],
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2ctlAzhh_qBa"
      },
      "source": [
        "Now head on over to your Labelbox account! You should see a new project by the name you specified above, and when you hit Start Labeling, you should see all the predicted masks rendered.\n",
        "\n",
        "Using the tools in the image editor, you can then modify or review the masks."
      ],
      "cell_type": "markdown"
    }
  ]
}